{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "data from http://jmcauley.ucsd.edu/data/tradesy/\n",
    "```\n",
    "wget http://jmcauley.ucsd.edu/data/tradesy/tradesy.json.gz\n",
    "wget http://jmcauley.ucsd.edu/data/tradesy/tradesy_item_urls.json.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import gzip\n",
    "import struct\n",
    "from collections import defaultdict\n",
    "import gensim as gs\n",
    "import numpy as np\n",
    "try:\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        print(\"notebook\")\n",
    "        from tqdm import tqdm_notebook as tqdm\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "except (NameError, RuntimeError):\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_data = eval(gzip.open(\"./tradesy.json.gz\", 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(user_data, max_uid=1000000):\n",
    "    line_list =[]\n",
    "    user_list = []\n",
    "    item_dic = {}\n",
    "    few_buyers =[]\n",
    "    data = defaultdict(set)\n",
    "    for d  in user_data:\n",
    "        user_id = d[\"uid\"]\n",
    "        items = d[\"lists\"][\"bought\"]\n",
    "        \n",
    "        item_list = [int(i) for i in items]\n",
    "        if item_list:\n",
    "            max_i = max(item_list)\n",
    "            if max_i < max_uid:\n",
    "                user_list.append(user_id)\n",
    "                line_list.append(items)\n",
    "            \n",
    "    dictionary = gs.corpora.Dictionary(line_list)\n",
    "    dictionary.filter_extremes(no_below=1)\n",
    "    dictionary.compactify()\n",
    "    for u, items in zip(user_list, line_list):\n",
    "        data[u].update([dictionary.token2id[item] for item in items if item in dictionary.token2id])\n",
    "    for u,i in data.items():\n",
    "        if len(i) < 5:   # 5 same as the paper's\n",
    "            few_buyers.append(u)\n",
    "    for u in few_buyers:\n",
    "        del data[u]\n",
    "    d = {}\n",
    "    user_list = []\n",
    "    for idx,(u,i) in enumerate(data.items()):\n",
    "        d[idx] = i\n",
    "        user_list.append(u)\n",
    "    user_count = len(data.keys())\n",
    "    item_count = len(dictionary)\n",
    "    return user_count, item_count, d, dictionary, user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test(data):\n",
    "    user_test = dict()\n",
    "    for u, i_list in data.items():\n",
    "        if i_list:\n",
    "            user_test[u] = np.random.choice(list(i_list))\n",
    "    return user_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item count:  32807\n",
      "user count:  1076\n"
     ]
    }
   ],
   "source": [
    "max_uid=1000000\n",
    "user_count, item_count, data, dictionary,u_list  = load_data(user_data, max_uid)\n",
    "print(\"item count: \", item_count)\n",
    "print(\"user count: \", user_count)\n",
    "ui_test = generate_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file_name = \"./items.pickle\"\n",
    "#with open(file_name ,mode='wb') as f:\n",
    "#    pickle.dump(list(dictionary.token2id.keys()), f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readImageFeatures(path, dictionary):\n",
    "    f = open(path, 'rb')\n",
    "    imgs = {}\n",
    "    uids = dictionary.token2id.keys()\n",
    "    count = 0\n",
    "    while f:\n",
    "        userId = f.read(10)\n",
    "        userId = userId.strip()\n",
    "        if userId == '':\n",
    "            break\n",
    "        uid =  userId.decode('ascii')\n",
    "        if uid in uids:\n",
    "            feature = [struct.unpack('f', f.read(4)) for _ in range(4096)]\n",
    "            imgs[dictionary.token2id[uid]] = feature\n",
    "            count += 1\n",
    "            if count == len(uids):\n",
    "                break\n",
    "        else:\n",
    "            f.read(4*4096)\n",
    "            \n",
    "    file_name = \"./images.pickle\"\n",
    "    with open(file_name ,mode='wb') as f:\n",
    "        pickle.dump(image_features, f, protocol=4)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#image_features = readImageFeatures(\"./image_features_tradesy.b\", dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = \"./images.pickle\"\n",
    "with open(file_name, mode='rb') as f:\n",
    "          image_features =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32807"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_features[13348])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def uniform_sample_batch(train_ratings, item_count, image_features, sample_count=20000, batch_size=5):\n",
    "    for i in range(sample_count):\n",
    "        t = []\n",
    "        iv = []\n",
    "        jv = []\n",
    "        for b in range(batch_size):\n",
    "            u = random.sample(train_ratings.keys(), 1)[0]\n",
    "            i = random.sample(train_ratings[u], 1)[0]\n",
    "            j = random.randint(0, item_count-1)\n",
    "            while j in train_ratings[u]:\n",
    "                j = random.randint(0, item_count-1)\n",
    "            t.append([u, i, j])\n",
    "            iv.append(image_features[i])\n",
    "            jv.append(image_features[j])\n",
    "        yield np.asarray(t), np.hstack(tuple(iv)), np.hstack(tuple(jv))\n",
    "\n",
    "def test_batch_generator_by_user(train_ratings, test_ratings, item_count, image_features, n_user=10):  \n",
    "    for u in np.random.choice(list(test_ratings.keys()), n_user):\n",
    "        i = test_ratings[u]\n",
    "        t = []\n",
    "        ilist = []\n",
    "        jlist = []\n",
    "        for j in range(item_count):\n",
    "            if j != test_ratings[u] and not (j in train_ratings[u]):\n",
    "                t.append([u, i, j])\n",
    "                ilist.append(image_features[i])\n",
    "                jlist.append(image_features[j])\n",
    "        yield np.asarray(t), np.hstack(tuple(ilist)), np.hstack(tuple(jlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, mean=0.0, stddev=0.01))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, mean=0.0, stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vbpr(user_count, item_count, hidden_dim=20, hidden_img_dim=128, learning_rate = 0.001,l2_regulization = 1.0):\n",
    "    image_dim = 4096\n",
    "    u = tf.placeholder(tf.int32, [None])\n",
    "    i = tf.placeholder(tf.int32, [None])\n",
    "    j = tf.placeholder(tf.int32, [None])\n",
    "    iv = tf.placeholder(tf.float32, [4096, None])\n",
    "    jv = tf.placeholder(tf.float32, [4096, None])\n",
    "    \n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "        user_emb_w = weight_variable([user_count+1, hidden_dim])\n",
    "        user_img_w = weight_variable([user_count+1, hidden_img_dim])\n",
    "        item_emb_w = weight_variable([item_count+1, hidden_dim])\n",
    "        item_b = bias_variable([item_count+1, 1])\n",
    "        \n",
    "        u_emb = tf.nn.embedding_lookup(user_emb_w, u)\n",
    "        u_img = tf.nn.embedding_lookup(user_img_w, u)\n",
    "        \n",
    "        i_emb = tf.nn.embedding_lookup(item_emb_w, i)\n",
    "        i_b = tf.nn.embedding_lookup(item_b, i)\n",
    "        j_emb = tf.nn.embedding_lookup(item_emb_w, j)\n",
    "        j_b = tf.nn.embedding_lookup(item_b, j)\n",
    "    \n",
    "    with tf.device(\"/cpu:0\"):\n",
    "   \n",
    "        img_emb_w = weight_variable([4096, hidden_img_dim])\n",
    "\n",
    "        img_i_j = tf.matmul(tf.transpose(iv - jv),img_emb_w)\n",
    "\n",
    "        x = i_b - j_b + tf.reduce_sum(tf.matmul(u_emb, tf.transpose(i_emb - j_emb)), 1, keep_dims=True) +\\\n",
    "            tf.reduce_sum(tf.matmul(u_img, tf.transpose(img_i_j)),1, keep_dims=True)\n",
    "\n",
    "        auc = tf.reduce_mean(tf.to_float(x > 0))\n",
    "\n",
    "        l2_norm = tf.add_n([\n",
    "                tf.reduce_sum(tf.norm(u_emb)), \n",
    "                tf.reduce_sum(tf.norm(u_img)),\n",
    "                tf.reduce_sum(tf.norm(i_emb)),\n",
    "                tf.reduce_sum(tf.norm(j_emb)),\n",
    "                tf.reduce_sum(tf.norm(img_emb_w)),\n",
    "                tf.reduce_sum(tf.norm(i_b)),\n",
    "                tf.reduce_sum(tf.norm(j_b))\n",
    "            ])\n",
    "\n",
    "        loss = l2_regulization * l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(x)))\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return u, i, j, iv, jv, loss, auc, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_n_user = 10 #len(ui_test)\n",
    "sample_count = 500\n",
    "n_epoch = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  train_loss: 4.55186868095\n",
      "epoch  1  train_loss: 1.45956698442\n",
      "epoch  2  train_loss: 1.27494984078\n",
      "epoch  3  train_loss: 1.18805670714\n",
      "epoch  4  train_loss: 1.10765860271\n",
      "epoch  5  train_loss: 1.0338605653\n",
      "epoch  6  train_loss: 0.973891965866\n",
      "epoch  7  train_loss: 0.928053256631\n",
      "epoch  8  train_loss: 0.900440600753\n",
      "epoch  9  train_loss: 0.886892068744\n",
      "epoch  10  train_loss: 0.878287965417\n",
      "test_loss:  14.3501008034  auc:  0.499853638003\n",
      "epoch  11  train_loss: 0.872071281791\n",
      "epoch  12  train_loss: 0.861356754899\n",
      "epoch  13  train_loss: 0.857188465357\n",
      "epoch  14  train_loss: 0.851950132012\n",
      "epoch  15  train_loss: 0.846854774833\n",
      "epoch  16  train_loss: 0.842722093821\n",
      "epoch  17  train_loss: 0.838809738874\n",
      "epoch  18  train_loss: 0.83522105515\n",
      "epoch  19  train_loss: 0.831964465499\n",
      "epoch  20  train_loss: 0.827382264137\n",
      "test_loss:  10.7508340836  auc:  0.5\n",
      "epoch  21  train_loss: 0.824874441266\n",
      "epoch  22  train_loss: 0.823237832308\n",
      "epoch  23  train_loss: 0.82103567636\n",
      "epoch  24  train_loss: 0.817990787625\n",
      "epoch  25  train_loss: 0.816007965684\n",
      "epoch  26  train_loss: 0.812893668294\n",
      "epoch  27  train_loss: 0.810264744163\n",
      "epoch  28  train_loss: 0.80887407124\n",
      "epoch  29  train_loss: 0.806616700292\n",
      "epoch  30  train_loss: 0.802834144592\n",
      "test_loss:  8.64914608002  auc:  0.6\n",
      "epoch  31  train_loss: 0.802843293071\n",
      "epoch  32  train_loss: 0.801141381145\n",
      "epoch  33  train_loss: 0.799015294909\n",
      "epoch  34  train_loss: 0.799755276442\n",
      "epoch  35  train_loss: 0.798075442433\n",
      "epoch  36  train_loss: 0.795361359358\n",
      "epoch  37  train_loss: 0.795314954877\n",
      "epoch  38  train_loss: 0.792651498437\n",
      "epoch  39  train_loss: 0.793516252398\n",
      "epoch  40  train_loss: 0.792371423125\n",
      "test_loss:  8.25593738556  auc:  0.400115850125\n",
      "epoch  41  train_loss: 0.789323064089\n",
      "epoch  42  train_loss: 0.78904938066\n",
      "epoch  43  train_loss: 0.788348747969\n",
      "epoch  44  train_loss: 0.785607101679\n",
      "epoch  45  train_loss: 0.785764744401\n",
      "epoch  46  train_loss: 0.784955799699\n",
      "epoch  47  train_loss: 0.783515760779\n",
      "epoch  48  train_loss: 0.783860464692\n",
      "epoch  49  train_loss: 0.782106203794\n",
      "epoch  50  train_loss: 0.782870913863\n",
      "test_loss:  7.28013501167  auc:  0.4\n",
      "epoch  51  train_loss: 0.780050424218\n",
      "epoch  52  train_loss: 0.779221687913\n",
      "epoch  53  train_loss: 0.779290016055\n",
      "epoch  54  train_loss: 0.778879113197\n",
      "epoch  55  train_loss: 0.778199384689\n",
      "epoch  56  train_loss: 0.777038308859\n",
      "epoch  57  train_loss: 0.776261637807\n",
      "epoch  58  train_loss: 0.776168239474\n",
      "epoch  59  train_loss: 0.773865797877\n",
      "epoch  60  train_loss: 0.774935006142\n",
      "test_loss:  6.5868309021  auc:  0.5\n",
      "epoch  61  train_loss: 0.774055376768\n",
      "epoch  62  train_loss: 0.773399731278\n",
      "epoch  63  train_loss: 0.773697006941\n",
      "epoch  64  train_loss: 0.772270044327\n",
      "epoch  65  train_loss: 0.771966649771\n",
      "epoch  66  train_loss: 0.771104996085\n",
      "epoch  67  train_loss: 0.77046062243\n",
      "epoch  68  train_loss: 0.770659217119\n",
      "epoch  69  train_loss: 0.770919460654\n",
      "epoch  70  train_loss: 0.769139047503\n",
      "test_loss:  6.18688902855  auc:  0.299844443798\n",
      "epoch  71  train_loss: 0.768052188158\n",
      "epoch  72  train_loss: 0.768163476229\n",
      "epoch  73  train_loss: 0.768032965183\n",
      "epoch  74  train_loss: 0.767450358987\n",
      "epoch  75  train_loss: 0.766774739504\n",
      "epoch  76  train_loss: 0.766765889764\n",
      "epoch  77  train_loss: 0.765049230337\n",
      "epoch  78  train_loss: 0.765785995722\n",
      "epoch  79  train_loss: 0.765913382173\n",
      "epoch  80  train_loss: 0.765024332881\n",
      "test_loss:  6.13339705467  auc:  0.300411560293\n",
      "epoch  81  train_loss: 0.764196122766\n",
      "epoch  82  train_loss: 0.765000305653\n",
      "epoch  83  train_loss: 0.763877060175\n",
      "epoch  84  train_loss: 0.763516152501\n",
      "epoch  85  train_loss: 0.763372970581\n",
      "epoch  86  train_loss: 0.764142128468\n",
      "epoch  87  train_loss: 0.763656624556\n",
      "epoch  88  train_loss: 0.762653950572\n",
      "epoch  89  train_loss: 0.761890776038\n",
      "epoch  90  train_loss: 0.762016815782\n",
      "test_loss:  6.1220202446  auc:  0.699594426155\n",
      "epoch  91  train_loss: 0.761534161568\n",
      "epoch  92  train_loss: 0.761792955995\n",
      "epoch  93  train_loss: 0.76197782743\n",
      "epoch  94  train_loss: 0.760363932133\n",
      "epoch  95  train_loss: 0.761198670983\n",
      "epoch  96  train_loss: 0.761150910735\n",
      "epoch  97  train_loss: 0.759895967007\n",
      "epoch  98  train_loss: 0.760786545396\n",
      "epoch  99  train_loss: 0.759603308558\n",
      "epoch  100  train_loss: 0.76035799849\n",
      "test_loss:  5.99539313316  auc:  0.5\n",
      "epoch  101  train_loss: 0.760078629613\n",
      "epoch  102  train_loss: 0.759457491398\n",
      "epoch  103  train_loss: 0.759431234956\n",
      "epoch  104  train_loss: 0.759373944163\n",
      "epoch  105  train_loss: 0.758877579927\n",
      "epoch  106  train_loss: 0.758793459296\n",
      "epoch  107  train_loss: 0.759106489897\n",
      "epoch  108  train_loss: 0.758221271873\n",
      "epoch  109  train_loss: 0.758763391256\n",
      "epoch  110  train_loss: 0.757964681268\n",
      "test_loss:  5.18004951477  auc:  0.7\n",
      "epoch  111  train_loss: 0.757776694655\n",
      "epoch  112  train_loss: 0.757880234122\n",
      "epoch  113  train_loss: 0.757668097973\n",
      "epoch  114  train_loss: 0.757632313371\n",
      "epoch  115  train_loss: 0.75821670568\n",
      "epoch  116  train_loss: 0.757318986654\n",
      "epoch  117  train_loss: 0.757488284588\n",
      "epoch  118  train_loss: 0.756935085058\n",
      "epoch  119  train_loss: 0.757486855507\n",
      "epoch  120  train_loss: 0.757395162106\n",
      "test_loss:  5.03537271023  auc:  0.4\n",
      "epoch  121  train_loss: 0.756698427796\n",
      "epoch  122  train_loss: 0.756473316908\n",
      "epoch  123  train_loss: 0.756246124625\n",
      "epoch  124  train_loss: 0.757068079233\n",
      "epoch  125  train_loss: 0.756468162894\n",
      "epoch  126  train_loss: 0.756688010573\n",
      "epoch  127  train_loss: 0.75658850801\n",
      "epoch  128  train_loss: 0.756439154267\n",
      "epoch  129  train_loss: 0.756965666294\n",
      "epoch  130  train_loss: 0.755963410616\n",
      "test_loss:  4.87153720856  auc:  0.619167682528\n",
      "epoch  131  train_loss: 0.756115969896\n",
      "epoch  132  train_loss: 0.756137017608\n",
      "epoch  133  train_loss: 0.75557580328\n",
      "epoch  134  train_loss: 0.756250051141\n",
      "epoch  135  train_loss: 0.755945531964\n",
      "epoch  136  train_loss: 0.755808864355\n",
      "epoch  137  train_loss: 0.755690369844\n",
      "epoch  138  train_loss: 0.756090421557\n",
      "epoch  139  train_loss: 0.755588736176\n",
      "epoch  140  train_loss: 0.755866023302\n",
      "test_loss:  4.74294922352  auc:  0.400204255828\n",
      "epoch  141  train_loss: 0.755352482438\n",
      "epoch  142  train_loss: 0.756065401077\n",
      "epoch  143  train_loss: 0.755341554761\n",
      "epoch  144  train_loss: 0.755769227505\n",
      "epoch  145  train_loss: 0.755397646785\n",
      "epoch  146  train_loss: 0.755620086193\n",
      "epoch  147  train_loss: 0.755225576401\n",
      "epoch  148  train_loss: 0.755582979321\n",
      "epoch  149  train_loss: 0.75559303093\n",
      "epoch  150  train_loss: 0.75573233974\n",
      "test_loss:  4.72337296009  auc:  0.299993902445\n",
      "epoch  151  train_loss: 0.754648201704\n",
      "epoch  152  train_loss: 0.755483780622\n",
      "epoch  153  train_loss: 0.755075448751\n",
      "epoch  154  train_loss: 0.755215902209\n",
      "epoch  155  train_loss: 0.755327128649\n",
      "epoch  156  train_loss: 0.755164091706\n",
      "epoch  157  train_loss: 0.75574146378\n",
      "epoch  158  train_loss: 0.75480692625\n",
      "epoch  159  train_loss: 0.755068316936\n",
      "epoch  160  train_loss: 0.755235246778\n",
      "test_loss:  5.4604719162  auc:  0.499996948242\n",
      "epoch  161  train_loss: 0.755025872469\n",
      "epoch  162  train_loss: 0.754988594294\n",
      "epoch  163  train_loss: 0.754822089791\n",
      "epoch  164  train_loss: 0.754589172721\n",
      "epoch  165  train_loss: 0.754944054008\n",
      "epoch  166  train_loss: 0.755239454985\n",
      "epoch  167  train_loss: 0.754724063516\n",
      "epoch  168  train_loss: 0.754961717725\n",
      "epoch  169  train_loss: 0.754901220918\n",
      "epoch  170  train_loss: 0.755090693951\n",
      "test_loss:  5.62069787979  auc:  0.5\n",
      "epoch  171  train_loss: 0.754886588335\n",
      "epoch  172  train_loss: 0.754737893224\n",
      "epoch  173  train_loss: 0.754861892223\n",
      "epoch  174  train_loss: 0.754454542518\n",
      "epoch  175  train_loss: 0.754280818701\n",
      "epoch  176  train_loss: 0.754907919526\n",
      "epoch  177  train_loss: 0.755008227706\n",
      "epoch  178  train_loss: 0.754471042514\n",
      "epoch  179  train_loss: 0.754685048819\n",
      "epoch  180  train_loss: 0.75473974824\n",
      "test_loss:  5.03701350689  auc:  0.5\n",
      "epoch  181  train_loss: 0.75470096159\n",
      "epoch  182  train_loss: 0.754272902727\n",
      "epoch  183  train_loss: 0.754572250247\n",
      "epoch  184  train_loss: 0.754735530019\n",
      "epoch  185  train_loss: 0.754979781747\n",
      "epoch  186  train_loss: 0.754675460815\n",
      "epoch  187  train_loss: 0.754739643574\n",
      "epoch  188  train_loss: 0.754906754613\n",
      "epoch  189  train_loss: 0.754393855453\n",
      "epoch  190  train_loss: 0.755045837641\n",
      "test_loss:  5.70637602806  auc:  0.300003048595\n",
      "epoch  191  train_loss: 0.754459308147\n",
      "epoch  192  train_loss: 0.75516818738\n",
      "epoch  193  train_loss: 0.755111127973\n",
      "epoch  194  train_loss: 0.754367771626\n",
      "epoch  195  train_loss: 0.754584186435\n",
      "epoch  196  train_loss: 0.754676766872\n",
      "epoch  197  train_loss: 0.754441411734\n",
      "epoch  198  train_loss: 0.754754483104\n",
      "epoch  199  train_loss: 0.75430606091\n",
      "epoch  200  train_loss: 0.754852046967\n",
      "test_loss:  5.00435209274  auc:  0.6\n",
      "epoch  201  train_loss: 0.754232726693\n",
      "epoch  202  train_loss: 0.75466115725\n",
      "epoch  203  train_loss: 0.754564222336\n",
      "epoch  204  train_loss: 0.754355253339\n",
      "epoch  205  train_loss: 0.754464966178\n",
      "epoch  206  train_loss: 0.754367808342\n",
      "epoch  207  train_loss: 0.754743966699\n",
      "epoch  208  train_loss: 0.754645609021\n",
      "epoch  209  train_loss: 0.754498191953\n",
      "epoch  210  train_loss: 0.754141285419\n",
      "test_loss:  4.9579859972  auc:  0.6\n",
      "epoch  211  train_loss: 0.754664983511\n",
      "epoch  212  train_loss: 0.754170681\n",
      "epoch  213  train_loss: 0.75484391284\n",
      "epoch  214  train_loss: 0.754757648826\n",
      "epoch  215  train_loss: 0.754726650119\n",
      "epoch  216  train_loss: 0.754473827004\n",
      "epoch  217  train_loss: 0.755245566607\n",
      "epoch  218  train_loss: 0.754741965771\n",
      "epoch  219  train_loss: 0.754485078096\n",
      "epoch  220  train_loss: 0.754245342135\n",
      "test_loss:  5.10886983871  auc:  0.5\n",
      "epoch  221  train_loss: 0.754787197828\n",
      "epoch  222  train_loss: 0.754548056602\n",
      "epoch  223  train_loss: 0.754276419997\n",
      "epoch  224  train_loss: 0.754764637232\n",
      "epoch  225  train_loss: 0.754152840734\n",
      "epoch  226  train_loss: 0.754458995819\n",
      "epoch  227  train_loss: 0.754019356847\n",
      "epoch  228  train_loss: 0.754633459449\n",
      "epoch  229  train_loss: 0.75472730422\n",
      "epoch  230  train_loss: 0.754501450181\n",
      "test_loss:  5.10471081734  auc:  0.700006097189\n",
      "epoch  231  train_loss: 0.754337149143\n",
      "epoch  232  train_loss: 0.754840057254\n",
      "epoch  233  train_loss: 0.754639362454\n",
      "epoch  234  train_loss: 0.754569606423\n",
      "epoch  235  train_loss: 0.754495764732\n",
      "epoch  236  train_loss: 0.754445832133\n",
      "epoch  237  train_loss: 0.754824209094\n",
      "epoch  238  train_loss: 0.754087656379\n",
      "epoch  239  train_loss: 0.755107571959\n",
      "epoch  240  train_loss: 0.75426056993\n",
      "test_loss:  4.7524917841  auc:  0.4\n",
      "epoch  241  train_loss: 0.754646789074\n",
      "epoch  242  train_loss: 0.754272127509\n",
      "epoch  243  train_loss: 0.754327928662\n",
      "epoch  244  train_loss: 0.754627146721\n",
      "epoch  245  train_loss: 0.75429470396\n",
      "epoch  246  train_loss: 0.754371336699\n",
      "epoch  247  train_loss: 0.754310835481\n",
      "epoch  248  train_loss: 0.754445943475\n",
      "epoch  249  train_loss: 0.754116116285\n",
      "epoch  250  train_loss: 0.754210072637\n",
      "test_loss:  4.42300856113  auc:  0.400003048595\n",
      "epoch  251  train_loss: 0.755232193589\n",
      "epoch  252  train_loss: 0.754486697912\n",
      "epoch  253  train_loss: 0.754958698034\n",
      "epoch  254  train_loss: 0.754705694914\n",
      "epoch  255  train_loss: 0.75424014318\n",
      "epoch  256  train_loss: 0.754723168254\n",
      "epoch  257  train_loss: 0.75467854023\n",
      "epoch  258  train_loss: 0.754523171186\n",
      "epoch  259  train_loss: 0.754077922225\n",
      "epoch  260  train_loss: 0.754475115538\n",
      "test_loss:  5.29974496365  auc:  0.3\n",
      "epoch  261  train_loss: 0.753893754363\n",
      "epoch  262  train_loss: 0.754527291417\n",
      "epoch  263  train_loss: 0.754814288735\n",
      "epoch  264  train_loss: 0.754789937496\n",
      "epoch  265  train_loss: 0.754417670369\n",
      "epoch  266  train_loss: 0.754166974664\n",
      "epoch  267  train_loss: 0.754113888979\n",
      "epoch  268  train_loss: 0.753921193719\n",
      "epoch  269  train_loss: 0.754942275286\n",
      "epoch  270  train_loss: 0.754356732726\n",
      "test_loss:  5.55687913895  auc:  0.5\n",
      "epoch  271  train_loss: 0.754627019525\n",
      "epoch  272  train_loss: 0.754460795403\n",
      "epoch  273  train_loss: 0.754450816751\n",
      "epoch  274  train_loss: 0.754389649034\n",
      "epoch  275  train_loss: 0.754250011683\n",
      "epoch  276  train_loss: 0.75460951972\n",
      "epoch  277  train_loss: 0.754484224796\n",
      "epoch  278  train_loss: 0.754113321304\n",
      "epoch  279  train_loss: 0.754369196653\n",
      "epoch  280  train_loss: 0.754168761969\n",
      "test_loss:  4.48760025501  auc:  0.5\n",
      "epoch  281  train_loss: 0.754327698946\n",
      "epoch  282  train_loss: 0.754885866523\n",
      "epoch  283  train_loss: 0.75457254827\n",
      "epoch  284  train_loss: 0.754084486008\n",
      "epoch  285  train_loss: 0.755006585956\n",
      "epoch  286  train_loss: 0.754026539445\n",
      "epoch  287  train_loss: 0.754471057177\n",
      "epoch  288  train_loss: 0.75445102942\n",
      "epoch  289  train_loss: 0.754446504951\n",
      "epoch  290  train_loss: 0.754082410812\n",
      "test_loss:  5.31020059586  auc:  0.6\n",
      "epoch  291  train_loss: 0.754087631583\n",
      "epoch  292  train_loss: 0.754576019049\n",
      "epoch  293  train_loss: 0.754641918182\n",
      "epoch  294  train_loss: 0.754809195042\n",
      "epoch  295  train_loss: 0.754350982666\n",
      "epoch  296  train_loss: 0.754380879045\n",
      "epoch  297  train_loss: 0.754665113688\n",
      "epoch  298  train_loss: 0.754259193063\n",
      "epoch  299  train_loss: 0.754811122298\n",
      "epoch  300  train_loss: 0.754524493575\n",
      "test_loss:  4.6289896965  auc:  0.8\n",
      "epoch  301  train_loss: 0.754645673752\n",
      "epoch  302  train_loss: 0.75441512835\n",
      "epoch  303  train_loss: 0.754050248265\n",
      "epoch  304  train_loss: 0.754927089453\n",
      "epoch  305  train_loss: 0.754013808727\n",
      "epoch  306  train_loss: 0.754485524774\n",
      "epoch  307  train_loss: 0.754578762889\n",
      "epoch  308  train_loss: 0.754211931944\n",
      "epoch  309  train_loss: 0.754305969715\n",
      "epoch  310  train_loss: 0.754493865609\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    with tf.variable_scope('vbpr'):\n",
    "        u, i, j, iv, jv, loss, auc, train_op = vbpr(user_count, item_count,learning_rate = 0.0001)\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "        \n",
    "        _loss_train = 0.0\n",
    "        for d,i_img,j_img  in uniform_sample_batch(data, item_count, image_features, sample_count=sample_count):\n",
    "            _loss, _ = session.run([loss, train_op], feed_dict={\n",
    "                    u:d[:,0], i:d[:,1], j:d[:,2], iv: i_img, jv: j_img\n",
    "                })\n",
    "            _loss_train += _loss\n",
    "            \n",
    "        print(\"epoch \", epoch, \" train_loss:\", _loss_train/sample_count)\n",
    "        \n",
    "        if epoch % 10 == 0 and epoch != 0:\n",
    "            _auc_all = 0.0\n",
    "            _loss_test = 0.0\n",
    "            for d,i_img,j_img in tqdm(test_batch_generator_by_user(data, ui_test, item_count, image_features, n_user=test_n_user)):\n",
    "                _loss, _auc = session.run([loss, auc], feed_dict={\n",
    "                        u:d[:,0], i:d[:,1], j:d[:,2], iv: i_img, jv: j_img\n",
    "                })\n",
    "                _loss_test += _loss\n",
    "                _auc_all += _auc\n",
    "            print( \"test_loss: \", _loss_test/test_n_user, \" auc: \", _auc_all/test_n_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (py3.6)",
   "language": "",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "01d5964d86da496f85167d8c926fa611": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "167b085f0b8d4e43ab086bf44629204e": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "17e263b08ab94f2db32b758b0caceaf9": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "1ae63f5e06ed498080edd122c3719d75": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "24acf9abc96445adb2925ea7359baa55": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "29e7731d2d804e25ba95d09e0f5f2127": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "2c62b83b99a0465288de051f0ab39ca6": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "30931ab2f73644d7b493c2ac351c08db": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "443eff14ff0e4c75ba01688d0f7037ae": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "4be26ce71bd047d1968dcd1bc4794478": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "534e9ee5ed6342d0ae68f96e2ccc932d": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "55dda10aa2db47a5b126402e61f67187": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "58e158f24a7d474ab304d1a0f3505279": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "5d711b574dcb46e29b5282e5e9063e2b": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "69c4700c28a44aac87ae7049fe232b67": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "7455c17aa9e9431f8e4bf4d1c0e13a59": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "7752707f397142508997f7ffbc7fc01c": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "839119c0bd974a12bfee1de862fe7c74": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "882e885e1b824a1a9db8991d0e0a7dc7": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "8946dd723b244065b71dc42a4935eece": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "8a756d1e7b424eab9832e9a88e052e83": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "98e26a8650644354a401497e92a37080": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "9d180b73f1fe401fbf6b912f0e517b54": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "9f3d561f705b411e8629e936b148c93d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "a18fcf23ed394164827b02e2f8f09897": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "a873e40e09f44a709c5a95b9969f3049": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "a8edf315cc5e4c4ea556530a622ace3b": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "bd032fa36ceb448c97bd96adfbe17b24": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "becd42671b984a6cbf0bb5ccd3226da6": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "c105f644e3744051a0733a1c2a1b167f": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "c9722b02eef5441c8ecd265af4380e5b": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e71ddf1a59a34bf0af23f42c3fe449c7": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "ef80478b59904e318714cd0818b36e9e": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "fa5325afd05b4a38ac5113022cec85a1": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "fb667c4fed144dbd85c9aa8949609811": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
