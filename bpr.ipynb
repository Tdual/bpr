{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data from https://grouplens.org/datasets/movielens/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import gensim as gs\n",
    "try:\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        from tqdm import tqdm_notebook as tqdm\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "except (NameError, RuntimeError):\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    data = defaultdict(set)\n",
    "    max_u_id = -1\n",
    "    max_i_id = -1\n",
    "    with open(data_path, 'r') as f:\n",
    "        f.readline()\n",
    "        for idx, line in enumerate(f):\n",
    "            u, i, _, _ = line.split(\",\")\n",
    "            u = int(u)\n",
    "            i = int(i)\n",
    "            data[u].add(i)\n",
    "            max_u_id = max(u, max_u_id)\n",
    "            max_i_id = max(i, max_i_id)\n",
    "            if idx == 1000:\n",
    "                break\n",
    "    return max_u_id, max_i_id, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_data(data_path):\n",
    "    line_list =[]\n",
    "    user_list = []\n",
    "    item_dic = {}\n",
    "    few_buyers =[]\n",
    "    data = defaultdict(set)\n",
    "    with open(data_path, 'r') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line = line[:-1] # remove \\n\n",
    "            l = line.split(\",\")\n",
    "            user_id = l[0]\n",
    "            items = l[1:]\n",
    "            user_list.append(user_id)\n",
    "            line_list.append(items)\n",
    "    dictionary = gs.corpora.Dictionary(line_list)\n",
    "    for u, items in zip(user_list, line_list):\n",
    "        data[u].update([dictionary.token2id[item] for item in items])\n",
    "    for u,i in data.items():\n",
    "        if len(i) < 10:\n",
    "            few_buyers.append(u)\n",
    "    for u in few_buyers:\n",
    "        del data[u]\n",
    "    d = {}\n",
    "    user_list = []\n",
    "    for idx,(u,i) in enumerate(data.items()):\n",
    "        d[idx] = i\n",
    "        user_list.append(u)\n",
    "    user_count = len(data.keys())\n",
    "    item_count = len(dictionary)\n",
    "    return (user_count, item_count, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_count, item_count, data = map_data(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item count:  30572\n",
      "user count:  5922\n"
     ]
    }
   ],
   "source": [
    "print(\"item count: \", item_count)\n",
    "print(\"user count: \", user_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_test(data):\n",
    "    user_test = dict()\n",
    "    for u, i_list in data.items():\n",
    "        user_test[u] = np.random.choice(list(i_list))\n",
    "    return user_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_path = \"./ml-20m/ratings.csv\"\n",
    "#user_count, item_count, data = load_data(data_path)\n",
    "user_ratings_test = generate_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train_batch(data, user_ratings_test, item_count, batch_size=512):\n",
    "    t = []\n",
    "    for _ in range(batch_size):\n",
    "        u = np.random.choice(list(data.keys()))\n",
    "        i = np.random.choice(list(data[u]))\n",
    "        while i == user_ratings_test[u]:\n",
    "            i = np.random.choice(list(data[u]))\n",
    "        \n",
    "        j = np.random.randint(1, item_count+1)\n",
    "        while j in data[u]:\n",
    "            j = np.random.randint(1, item_count+1)\n",
    "        t.append([u, i, j])\n",
    "    return np.asarray(t)\n",
    "\n",
    "def generate_test_batch(user_ratings, user_ratings_test, item_count):\n",
    "    for u in np.random.choice(list(user_ratings.keys()),500):\n",
    "        t = []\n",
    "        i = user_ratings_test[u]\n",
    "        for j in range(1, item_count+1):\n",
    "            if not (j in user_ratings[u]):\n",
    "                t.append([u, i, j])\n",
    "        yield np.asarray(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, mean=0.0, stddev=0.01))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, mean=0.0, stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bpr(user_count, item_count, hidden_dim, batch_size=512):\n",
    "    \n",
    "    u = tf.placeholder(tf.int32, [None])\n",
    "    i = tf.placeholder(tf.int32, [None])\n",
    "    j = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    user_w = weight_variable([user_count+1, hidden_dim])\n",
    "    item_w = weight_variable([item_count+1, hidden_dim])\n",
    "    item_b = bias_variable([item_count+1, 1])\n",
    "        \n",
    "        \n",
    "    u_e = tf.nn.embedding_lookup(user_w, u)\n",
    "        \n",
    "    i_e = tf.nn.embedding_lookup(item_w, i)\n",
    "    i_b = tf.nn.embedding_lookup(item_b, i)\n",
    "        \n",
    "    j_e = tf.nn.embedding_lookup(item_w, j)\n",
    "    j_b = tf.nn.embedding_lookup(item_b, j)\n",
    "    \n",
    "    # MF \n",
    "    x = i_b - j_b + tf.reduce_sum(tf.matmul(u_e, tf.transpose((i_e - j_e))), 1, keep_dims=True)\n",
    "    \n",
    "    \n",
    "    auc_per_user = tf.reduce_mean(tf.cast(x > 0,\"float\"))\n",
    "    \n",
    "    l2_norm = tf.add_n([\n",
    "            tf.reduce_sum(tf.norm(u_e)), \n",
    "            tf.reduce_sum(tf.norm(i_e)),\n",
    "            tf.reduce_sum(tf.norm(j_e))\n",
    "        ])\n",
    "    \n",
    "    regu_rate = 0.0001\n",
    "    loss = - tf.reduce_mean(tf.log(tf.sigmoid(x))) + regu_rate * l2_norm\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "    return u, i, j, auc_per_user, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  0 , loss:  0.103536615314\n",
      "\n",
      "test loss:  0.00474381 , test auc:  0.872\n",
      "\n",
      "epoch:  1 , loss:  0.00158002658971\n",
      "\n",
      "test loss:  0.00472847 , test auc:  0.89\n",
      "\n",
      "epoch:  2 , loss:  0.000997846030397\n",
      "\n",
      "test loss:  0.00732075 , test auc:  0.894\n",
      "\n",
      "epoch:  3 , loss:  0.000909460627357\n",
      "\n",
      "test loss:  0.0069152 , test auc:  0.904\n",
      "\n",
      "epoch:  4 , loss:  0.000881892441044\n",
      "\n",
      "test loss:  0.00696584 , test auc:  0.856\n",
      "\n",
      "epoch:  5 , loss:  0.000846705647185\n",
      "\n",
      "test loss:  0.00704901 , test auc:  0.92\n",
      "\n",
      "epoch:  6 , loss:  0.000802511988761\n",
      "\n",
      "test loss:  0.00519197 , test auc:  0.912\n",
      "\n",
      "epoch:  7 , loss:  0.000777330828772\n",
      "\n",
      "test loss:  inf , test auc:  0.912\n",
      "\n",
      "epoch:  8 , loss:  0.000768964144139\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    u, i, j, auc_per_user, loss, train_op = bpr(user_count, item_count, 20)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        _batch_loss = 0\n",
    "        for index in tqdm(range(2000)): \n",
    "            uij = generate_train_batch(data, user_ratings_test, item_count)\n",
    "            _loss, _ = session.run([loss, train_op], feed_dict={u:uij[:,0], i:uij[:,1], j:uij[:,2]})\n",
    "            _batch_loss += _loss\n",
    "                   \n",
    "        print(\"epoch: \", epoch, \", loss: \", _batch_loss / (index+1))\n",
    "\n",
    "\n",
    "        _auc_sum = 0.0\n",
    "        user_count = 0\n",
    "        for t_uij in tqdm(generate_test_batch(data, user_ratings_test, item_count)):\n",
    "            _auc_per_user, _test_loss = session.run([auc_per_user, loss],feed_dict={u:t_uij[:,0], i:t_uij[:,1], j:t_uij[:,2]})\n",
    "            user_count += 1\n",
    "            _auc_sum += _auc_per_user\n",
    "            \n",
    "        _auc = _auc_sum/user_count # eq (1) in the paper\n",
    "            \n",
    "        print(\"test loss: \", _test_loss, \", test auc: \", _auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (py3.6)",
   "language": "",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "0295a4ca6a4c4db4be499336d2d5652a": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "0ddf3629ef354850bce4e85c4bc60a1a": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "0ff3300a49ad49828e39141b5a45005a": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "1148ca0217c64748ab2e45389b441adb": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "1648b145a46a4375a01799c0a387e5d0": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "261000a4df3446fea1578363595a7649": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "29b24f7d1d6f4f9c946262ecee842cbb": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "2a1965166dde47e7ad8f2208a2b36202": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "2f699b7097bd4328ab5af1cd5a6ee9de": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "41c1196f997b450388a741264bedbbc1": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "4239a036b4cb4eeea13826c85abc4989": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "5270744fa77a4c2ca3e32806a1deef47": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "600b8be4b4a5456aaa05875dfac34cbb": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "645e915d2bd24202ab1770d96821c679": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "93f1822203914380a8e22fcfc436bef2": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "98a6f3f81b2a4aeb9e4d221363e3b6d9": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "9e7cad069f1a46068e63d43e74b01a8d": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "9fe63e1b059a40c69d252736946e9d64": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "ad9e25d2563e427f927fb496a0f3b542": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "e06e5a4c6f234baa9a38123fdcc04a27": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "e341c6e5f1ae466f89d3108fe2170645": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "e4dc3767ba2b4333bff31e69dc82e385": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "ebef623523854d03b7ecaf0500b77802": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
